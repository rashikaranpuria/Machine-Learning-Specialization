{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Gaussian Mixture Models with EM\n",
    "\n",
    "In this assignment you will\n",
    "* implement the EM algorithm for a Gaussian mixture model\n",
    "* apply your implementation to cluster images\n",
    "* explore clustering results and interpret the output of the EM algorithm  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note to Amazon EC2 users**: To conserve memory, make sure to stop all the other notebooks before running this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This non-commercial license of GraphLab Create is assigned to abhishek.raj@iiitdmj.ac.in and will expire on May 23, 2017. For commercial licensing options, visit https://dato.com/buy/.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] graphlab.cython.cy_server: GraphLab Create v1.10.1 started. Logging: /tmp/graphlab_server_1468232986.log\n"
     ]
    }
   ],
   "source": [
    "import graphlab as gl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import copy\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing the EM algorithm for Gaussian mixture models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, you will implement the EM algorithm. We will take the following steps:\n",
    "\n",
    "- Create some synthetic data.\n",
    "- Provide a log likelihood function for this model.\n",
    "- Implement the EM algorithm.\n",
    "- Visualize the progress of the parameters during the course of running EM.\n",
    "- Visualize the convergence of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "To help us develop and test our implementation, we will generate some observations from a mixture of Gaussians and then run our EM algorithm to discover the mixture components. We'll begin with a function to generate the data, and a quick plot to visualize its output for a 2-dimensional mixture of three Gaussians.\n",
    "\n",
    "Now we will create a function to generate data from a mixture of Gaussians model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def generate_MoG_data(num_data, means, covariances, weights):\n",
    "    \"\"\" Creates a list of data points \"\"\"\n",
    "    num_clusters = len(weights)\n",
    "    data = []\n",
    "    for i in range(num_data):\n",
    "        #  Use np.random.choice and weights to pick a cluster id greater than or equal to 0 and less than num_clusters.\n",
    "        k = np.random.choice(len(weights), 1, p=weights)[0]\n",
    "\n",
    "        # Use np.random.multivariate_normal to create data from this cluster\n",
    "        x = np.random.multivariate_normal(means[k], covariances[k])\n",
    "\n",
    "        data.append(x)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After specifying a particular set of clusters (so that the results are reproducible across assignments), we use the above function to generate a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "init_means = [\n",
    "    [5, 0], # mean of cluster 1\n",
    "    [1, 1], # mean of cluster 2\n",
    "    [0, 5]  # mean of cluster 3\n",
    "]\n",
    "init_covariances = [\n",
    "    [[.5, 0.], [0, .5]], # covariance of cluster 1\n",
    "    [[1., .7], [0, .7]], # covariance of cluster 2\n",
    "    [[.5, 0.], [0, .5]]  # covariance of cluster 3\n",
    "]\n",
    "init_weights = [1/4., 1/2., 1/4.]  # weights of each cluster\n",
    "\n",
    "# Generate data\n",
    "np.random.seed(4)\n",
    "data = generate_MoG_data(100, init_means, init_covariances, init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**: To verify your implementation above, make sure the following code does not return an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint passed!\n"
     ]
    }
   ],
   "source": [
    "assert len(data) == 100\n",
    "assert len(data[0]) == 2\n",
    "print 'Checkpoint passed!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the data you created above. The plot should be a scatterplot with 100 points that appear to roughly fall into three clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEKCAYAAADEovgeAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngvv/nXVsUX6adYgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f82b5630910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "d = np.vstack(data)\n",
    "plt.plot(d[:,0], d[:,1],'ko')\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log likelihood \n",
    "We provide a function to calculate log likelihood for mixture of Gaussians. The log likelihood quantifies the probability of observing a given set of data under a particular setting of the parameters in our model. We will use this to assess convergence of our EM algorithm; specifically, we will keep looping through EM update steps until the log likehood ceases to increase at a certain rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def log_sum_exp(Z):\n",
    "    \"\"\" Compute log(\\sum_i exp(Z_i)) for some array Z.\"\"\"\n",
    "    return np.max(Z) + np.log(np.sum(np.exp(Z - np.max(Z))))\n",
    "\n",
    "def loglikelihood(data, weights, means, covs):\n",
    "    \"\"\" Compute the loglikelihood of the data for a Gaussian mixture model with the given parameters. \"\"\"\n",
    "    num_clusters = len(means)\n",
    "    num_dim = len(data[0])\n",
    "    \n",
    "    ll = 0\n",
    "    for d in data:\n",
    "        \n",
    "        Z = np.zeros(num_clusters)\n",
    "        for k in range(num_clusters):\n",
    "            \n",
    "            # Compute (x-mu)^T * Sigma^{-1} * (x-mu)\n",
    "            delta = np.array(d) - means[k]\n",
    "            exponent_term = np.dot(delta.T, np.dot(np.linalg.inv(covs[k]), delta))\n",
    "            \n",
    "            # Compute loglikelihood contribution for this data point and this cluster\n",
    "            Z[k] += np.log(weights[k])\n",
    "            Z[k] -= 1/2. * (num_dim * np.log(2*np.pi) + np.log(np.linalg.det(covs[k])) + exponent_term)\n",
    "            \n",
    "        # Increment loglikelihood contribution of this data point across all clusters\n",
    "        ll += log_sum_exp(Z)\n",
    "        \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "You will now complete an implementation that can run EM on the data you just created. It uses the `loglikelihood` function we provided above.\n",
    "\n",
    "Fill in the places where you find ## YOUR CODE HERE. There are seven places in this function for you to fill in.\n",
    "\n",
    "Hint: Some useful functions\n",
    "\n",
    "* [multivariate_normal.pdf](http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.multivariate_normal.html): lets you compute the likelihood of seeing a data point in a multivariate Gaussian distribution.\n",
    "* [np.outer](http://docs.scipy.org/doc/numpy/reference/generated/numpy.outer.html): comes in handy when estimating the covariance matrix from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def EM(data, init_means, init_covariances, init_weights, maxiter=1000, thresh=1e-4):\n",
    "    \n",
    "    # Make copies of initial parameters, which we will update during each iteration\n",
    "    means = init_means[:]\n",
    "    covariances = init_covariances[:]\n",
    "    weights = init_weights[:]\n",
    "    \n",
    "    # Infer dimensions of dataset and the number of clusters\n",
    "    num_data = len(data)\n",
    "    num_dim = len(data[0])\n",
    "    num_clusters = len(means)\n",
    "    \n",
    "    # Initialize some useful variables\n",
    "    resp = np.zeros((num_data, num_clusters))\n",
    "    ll = loglikelihood(data, weights, means, covariances)\n",
    "    ll_trace = [ll]\n",
    "    \n",
    "    for i in range(maxiter):\n",
    "        if i % 5 == 0:\n",
    "            print(\"Iteration %s\" % i)\n",
    "        \n",
    "        # E-step: compute responsibilities\n",
    "        # Update resp matrix so that resp[j, k] is the responsibility of cluster k for data point j.\n",
    "        # Hint: To compute likelihood of seeing data point j given cluster k, use multivariate_normal.pdf.\n",
    "        for j in range(num_data):\n",
    "            for k in range(num_clusters):\n",
    "                # YOUR CODE HERE\n",
    "                resp[j, k] = multivariate_normal.pdf(data[j], means[k], covariances[k])\n",
    "        row_sums = resp.sum(axis=1)[:, np.newaxis]\n",
    "        resp = resp / row_sums # normalize over all possible cluster assignments\n",
    "\n",
    "        # M-step\n",
    "        # Compute the total responsibility assigned to each cluster, which will be useful when \n",
    "        # implementing M-steps below. In the lectures this is called N^{soft}\n",
    "        counts = np.sum(resp, axis=0)\n",
    "        \n",
    "        for k in range(num_clusters):\n",
    "            Nsoft = resp[ ,k].sum()\n",
    "            # Update the weight for cluster k using the M-step update rule for the cluster weight, \\hat{\\pi}_k.\n",
    "            # YOUR CODE HERE\n",
    "            weights[k] = Nsoft/num_data\n",
    "            \n",
    "            # Update means for cluster k using the M-step update rule for the mean variables.\n",
    "            # This will assign the variable means[k] to be our estimate for \\hat{\\mu}_k.\n",
    "            weighted_sum = 0\n",
    "            for j in range(num_data):\n",
    "                # YOUR CODE HERE\n",
    "                weighted_sum += resp[j,k]*data[j]\n",
    "            # YOUR CODE HERE\n",
    "            means[k] = weighted_sum/Nsoft\n",
    "            \n",
    "            # Update covariances for cluster k using the M-step update rule for covariance variables.\n",
    "            # This will assign the variable covariances[k] to be the estimate for \\hat{\\Sigma}_k.\n",
    "            weighted_sum = np.zeros((num_dim, num_dim))\n",
    "            for j in range(num_data):\n",
    "                # YOUR CODE HERE (Hint: Use np.outer on the data[j] and this cluster's mean)\n",
    "                weighted_sum += np.outer(data[j],means[k])*resp[j,k]\n",
    "            # YOUR CODE HERE\n",
    "            covariances[k] = weighted_sum/resp[ :k].sum()\n",
    "          \n",
    "        \n",
    "        # Compute the loglikelihood at this iteration\n",
    "        # YOUR CODE HERE\n",
    "        ll_latest = loglikelihood(data, weights, means, covariances)\n",
    "        ll_trace.append(ll_latest)\n",
    "        \n",
    "        # Check for convergence in log-likelihood and store\n",
    "        if (ll_latest - ll) < thresh and ll_latest > -np.inf:\n",
    "            break\n",
    "        ll = ll_latest\n",
    "    \n",
    "    if i % 5 != 0:\n",
    "        print(\"Iteration %s\" % i)\n",
    "    \n",
    "    out = {'weights': weights, 'means': means, 'covs': covariances, 'loglik': ll_trace, 'resp': resp}\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the implementation on the simulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll fit a mixture of Gaussians to this data using our implementation of the EM algorithm. As with k-means, it is important to ask how we obtain an initial configuration of mixing weights and component parameters. In this simple case, we'll take three random points to be the initial cluster means, use the empirical covariance of the data to be the initial covariance in each cluster (a clear overestimate), and set the initial mixing weights to be uniform across clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "array must not contain infs or NaNs",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-b2b1ae83c086>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Run EM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEM\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_means\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_covs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-11-9ec1092d0bc0>\u001b[0m in \u001b[0;36mEM\u001b[1;34m(data, init_means, init_covariances, init_weights, maxiter, thresh)\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_clusters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m                 \u001b[1;31m# YOUR CODE HERE\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m                 \u001b[0mresp\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultivariate_normal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmeans\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcovariances\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         \u001b[0mrow_sums\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mresp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresp\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mrow_sums\u001b[0m \u001b[1;31m# normalize over all possible cluster assignments\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/abc/anaconda3/envs/dato/lib/python2.7/site-packages/scipy/stats/_multivariate.pyc\u001b[0m in \u001b[0;36mpdf\u001b[1;34m(self, x, mean, cov, allow_singular)\u001b[0m\n\u001b[0;32m    497\u001b[0m         \u001b[0mdim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcov\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_parameters\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcov\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    498\u001b[0m         \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_quantiles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 499\u001b[1;33m         \u001b[0mpsd\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_PSD\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcov\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_singular\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_singular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    500\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_logpdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpsd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpsd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_pdet\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpsd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_squeeze_output\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/abc/anaconda3/envs/dato/lib/python2.7/site-packages/scipy/stats/_multivariate.pyc\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, M, cond, rcond, lower, check_finite, allow_singular)\u001b[0m\n\u001b[0;32m    141\u001b[0m         \u001b[1;31m# Note that eigh takes care of array conversion, chkfinite,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m         \u001b[1;31m# and assertion that the matrix is square.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m         \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meigh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlower\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    144\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m         \u001b[0meps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_eigvalsh_to_eps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrcond\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/abc/anaconda3/envs/dato/lib/python2.7/site-packages/scipy/linalg/decomp.pyc\u001b[0m in \u001b[0;36meigh\u001b[1;34m(a, b, lower, eigvals_only, overwrite_a, overwrite_b, turbo, eigvals, type, check_finite)\u001b[0m\n\u001b[0;32m    286\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \"\"\"\n\u001b[1;32m--> 288\u001b[1;33m     \u001b[0ma1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_asarray_validated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_finite\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_finite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0ma1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'expected square matrix'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/abc/anaconda3/envs/dato/lib/python2.7/site-packages/scipy/_lib/_util.pyc\u001b[0m in \u001b[0;36m_asarray_validated\u001b[1;34m(a, check_finite, sparse_ok, objects_ok, mask_ok, as_inexact)\u001b[0m\n\u001b[0;32m    185\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'masked arrays are not supported'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[0mtoarray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray_chkfinite\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcheck_finite\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m     \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mobjects_ok\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'O'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/abc/anaconda3/envs/dato/lib/python2.7/site-packages/numpy/lib/function_base.pyc\u001b[0m in \u001b[0;36masarray_chkfinite\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m   1020\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchar\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtypecodes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AllFloat'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misfinite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1021\u001b[0m         raise ValueError(\n\u001b[1;32m-> 1022\u001b[1;33m             \"array must not contain infs or NaNs\")\n\u001b[0m\u001b[0;32m   1023\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1024\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: array must not contain infs or NaNs"
     ]
    }
   ],
   "source": [
    "np.random.seed(4)\n",
    "\n",
    "# Initialization of parameters\n",
    "chosen = np.random.choice(len(data), 3, replace=False)\n",
    "initial_means = [data[x] for x in chosen]\n",
    "initial_covs = [np.cov(data, rowvar=0)] * 3\n",
    "initial_weights = [1/3.] * 3\n",
    "\n",
    "# Run EM \n",
    "results = EM(data, initial_means, initial_covs, initial_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint**. For this particular example, the EM algorithm is expected to terminate in 30 iterations. That is, the last line of the log should say \"Iteration 29\". If your function stopped too early or too late, you should re-visit your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our algorithm returns a dictionary with five elements: \n",
    "* 'loglik': a record of the log likelihood at each iteration\n",
    "* 'resp': the final responsibility matrix\n",
    "* 'means': a list of K means\n",
    "* 'covs': a list of K covariance matrices\n",
    "* 'weights': the weights corresponding to each model component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: What is the weight that EM assigns to the first component after running the above codeblock?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Using the same set of results, obtain the mean that EM assigns the second component. What is the mean in the first dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Using the same set of results, obtain the covariance that EM assigns the third component. What is the variance in the first dimension?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot progress of parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful feature of testing our implementation on low-dimensional simulated data is that we can easily visualize the results. \n",
    "\n",
    "We will use the following `plot_contours` function to visualize the Gaussian components over the data at three different points in the algorithm's execution:\n",
    "\n",
    "1. At initialization (using initial_mu, initial_cov, and initial_weights)\n",
    "2. After running the algorithm to completion \n",
    "3. After just 12 iterations (using parameters estimates returned when setting `maxiter=12`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.mlab as mlab\n",
    "def plot_contours(data, means, covs, title):\n",
    "    plt.figure()\n",
    "    plt.plot([x[0] for x in data], [y[1] for y in data],'ko') # data\n",
    "\n",
    "    delta = 0.025\n",
    "    k = len(means)\n",
    "    x = np.arange(-2.0, 7.0, delta)\n",
    "    y = np.arange(-2.0, 7.0, delta)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    col = ['green', 'red', 'indigo']\n",
    "    for i in range(k):\n",
    "        mean = means[i]\n",
    "        cov = covs[i]\n",
    "        sigmax = np.sqrt(cov[0][0])\n",
    "        sigmay = np.sqrt(cov[1][1])\n",
    "        sigmaxy = cov[0][1]/(sigmax*sigmay)\n",
    "        Z = mlab.bivariate_normal(X, Y, sigmax, sigmay, mean[0], mean[1], sigmaxy)\n",
    "        plt.contour(X, Y, Z, colors = col[i])\n",
    "        plt.title(title)\n",
    "    plt.rcParams.update({'font.size':16})\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters after initialization\n",
    "plot_contours(data, initial_means, initial_covs, 'Initial clusters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Parameters after running EM to convergence\n",
    "results = EM(data, initial_means, initial_covs, initial_weights)\n",
    "plot_contours(data, results['means'], results['covs'], 'Final clusters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the following code block to visualize the set of parameters we get after running EM for 12 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "results = ...\n",
    "\n",
    "plot_contours(data, results['means'], results['covs'], 'Clusters after 12 iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Plot the loglikelihood that is observed at each iteration. Is the loglikelihood plot monotonically increasing, monotonically decreasing, or neither [multiple choice]? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = EM(data, initial_means, initial_covs, initial_weights)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "loglikelihoods = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(range(len(loglikelihoods)), loglikelihoods, linewidth=4)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log-likelihood')\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Gaussian mixture model for image data\n",
    "\n",
    "Now that we're confident in our implementation of the EM algorithm, we'll apply it to cluster some more interesting data. In particular, we have a set of images that come from four categories: sunsets, rivers, trees and forests, and cloudy skies. For each image we are given the average intensity of its red, green, and blue pixels, so we have a 3-dimensional representation of our data. Our goal is to find a good clustering of these images using our EM implementation; ideally our algorithm would find clusters that roughly correspond to the four image categories.\n",
    "\n",
    "To begin with, we'll take a look at the data and get it in a form suitable for input to our algorithm. The data are provided in SFrame format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images = gl.SFrame('images.sf')\n",
    "gl.canvas.set_target('ipynb')\n",
    "import array\n",
    "images['rgb'] = images.pack_columns(['red', 'green', 'blue'])['X4']\n",
    "images.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to come up with initial estimates for the mixture weights and component parameters. Let's take three images to be our initial cluster centers, and let's initialize the covariance matrix of each cluster to be diagonal with each element equal to the sample variance from the full data. As in our test on simulated data, we'll start by assuming each mixture component has equal weight. \n",
    "\n",
    "This may take a few minutes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "# Initalize parameters\n",
    "init_means = [images['rgb'][x] for x in np.random.choice(len(images), 4, replace=False)]\n",
    "cov = np.diag([images['red'].var(), images['green'].var(), images['blue'].var()])\n",
    "init_covariances = [cov, cov, cov, cov]\n",
    "init_weights = [1/4., 1/4., 1/4., 1/4.]\n",
    "\n",
    "# Convert rgb data to numpy arrays\n",
    "img_data = [np.array(i) for i in images['rgb']]  \n",
    "\n",
    "# Run our EM algorithm on the image data using the above initializations. \n",
    "# This should converge in about 125 iterations\n",
    "out = EM(img_data, init_means, init_covariances, init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sections will evaluate the results by asking the following questions:\n",
    "\n",
    "* **Convergence**: How did the log likelihood change across iterations? Did the algorithm achieve convergence?\n",
    "* **Uncertainty**: How did cluster assignment and uncertainty evolve?\n",
    "* **Interpretability**: Can we view some example images from each cluster? Do these clusters correspond to known image categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating convergence\n",
    "\n",
    "Let's start by plotting the log likelihood at each iteration - we know that the EM algorithm guarantees that the log likelihood can only increase (or stay the same) after each iteration, so if our implementation is correct then we should see an increasing function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ll = out['loglik']\n",
    "plt.plot(range(len(ll)),ll,linewidth=4)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log-likelihood')\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log likelihood increases so quickly on the first few iterations that we can barely see the plotted line. Let's plot the log likelihood after the first three iterations to get a clearer view of what's going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(range(3,len(ll)),ll[3:],linewidth=4)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Log-likelihood')\n",
    "plt.rcParams.update({'font.size':16})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating uncertainty\n",
    "\n",
    "Next we'll explore the evolution of cluster assignment and uncertainty. Remember that the EM algorithm represents uncertainty about the cluster assignment of each data point through the responsibility matrix. Rather than making a 'hard' assignment of each data point to a single cluster, the algorithm computes the responsibility of each cluster for each data point, where the responsibility corresponds to our certainty that the observation came from that cluster. \n",
    "\n",
    "We can track the evolution of the responsibilities across iterations to see how these 'soft' cluster assignments change as the algorithm fits the Gaussian mixture model to the data; one good way to do this is to plot the data and color each point according to its cluster responsibilities. Our data are three-dimensional, which can make visualization difficult, so to make things easier we will plot the data using only two dimensions, taking just the [R G], [G B] or [R B] values instead of the full [R G B] measurement for each observation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import colorsys\n",
    "def plot_responsibilities_in_RB(img, resp, title):\n",
    "    N, K = resp.shape\n",
    "    \n",
    "    HSV_tuples = [(x*1.0/K, 0.5, 0.9) for x in range(K)]\n",
    "    RGB_tuples = map(lambda x: colorsys.hsv_to_rgb(*x), HSV_tuples)\n",
    "    \n",
    "    R = img['red']\n",
    "    B = img['blue']\n",
    "    resp_by_img_int = [[resp[n][k] for k in range(K)] for n in range(N)]\n",
    "    cols = [tuple(np.dot(resp_by_img_int[n], np.array(RGB_tuples))) for n in range(N)]\n",
    "\n",
    "    plt.figure()\n",
    "    for n in range(len(R)):\n",
    "        plt.plot(R[n], B[n], 'o', c=cols[n])\n",
    "    plt.title(title)\n",
    "    plt.xlabel('R value')\n",
    "    plt.ylabel('B value')\n",
    "    plt.rcParams.update({'font.size':16})\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will visualize what happens when each data has random responsibilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "N, K = out['resp'].shape\n",
    "random_resp = np.random.dirichlet(np.ones(K), N)\n",
    "plot_responsibilities_in_RB(images, random_resp, 'Random responsibilities')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the above plotting function to visualize the responsibilites after 1 iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = EM(img_data, init_means, init_covariances, init_weights, maxiter=1)\n",
    "plot_responsibilities_in_RB(images, out['resp'], 'After 1 iteration')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use the above plotting function to visualize the responsibilites after 20 iterations. We will see there are fewer unique colors; this indicates that there is more certainty that each point belongs to one of the four components in the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "out = EM(img_data, init_means, init_covariances, init_weights, maxiter=20)\n",
    "plot_responsibilities_in_RB(images, out['resp'], 'After 20 iterations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the responsibilities over time in [R B] space shows a meaningful change in cluster assignments over the course of the algorithm's execution. While the clusters look significantly better organized at the end of the algorithm than they did at the start, it appears from our plot that they are still not very well separated. We note that this is due in part our decision to plot 3D data in a 2D space; everything that was separated along the G axis is now \"squashed\" down onto the flat [R B] plane. If we were to plot the data in full [R G B] space, then we would expect to see further separation of the final clusters.  We'll explore the cluster interpretability more in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting each cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's dig into the clusters obtained from our EM implementation. Recall that our goal in this section is to cluster images based on their RGB values. We can evaluate the quality of our clustering by taking a look at a few images that 'belong' to each cluster. We hope to find that the clusters discovered by our EM algorithm correspond to different image categories - in this case, we know that our images came from four categories ('cloudy sky', 'rivers', 'sunsets', and 'trees and forests'), so we would expect to find that each component of our fitted mixture model roughly corresponds to one of these categories.\n",
    "\n",
    "If we want to examine some example images from each cluster, we first need to consider how we can determine cluster assignments of the images from our algorithm output. This was easy with k-means - every data point had a 'hard' assignment to a single cluster, and all we had to do was find the cluster center closest to the data point of interest. Here, our clusters are described by probability distributions (specifically, Gaussians) rather than single points, and our model maintains some uncertainty about the cluster assignment of each observation.\n",
    "\n",
    "One way to phrase the question of cluster assignment for mixture models is as follows: how do we calculate the distance of a point from a distribution? Note that simple Euclidean distance might not be appropriate since (non-scaled) Euclidean distance doesn't take direction into account.  For example, if a Gaussian mixture component is very stretched in one direction but narrow in another, then a data point one unit away along the 'stretched' dimension has much higher probability (and so would be thought of as closer) than a data point one unit away along the 'narrow' dimension. \n",
    "\n",
    "In fact, the correct distance metric to use in this case is known as [Mahalanobis distance](https://en.wikipedia.org/wiki/Mahalanobis_distance). For a Gaussian distribution, this distance is proportional to the square root of the negative log likelihood. This makes sense intuitively - reducing the Mahalanobis distance of an observation from a cluster is equivalent to increasing that observation's probability according to the Gaussian that is used to represent the cluster. This also means that we can find the cluster assignment of an observation by taking the Gaussian component for which that observation scores highest. We'll use this fact to find the top examples that are 'closest' to each cluster.\n",
    "\n",
    "__Quiz Question:__ Calculate the likelihood (score) of the first image in our data set (`images[0]`) under each Gaussian component through a call to `multivariate_normal.pdf`.  Given these values, what cluster assignment should we make for this image? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we calculate cluster assignments for the entire image dataset using the result of running EM for 20 iterations above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "means = out['means']\n",
    "covariances = out['covs']\n",
    "rgb = images['rgb']\n",
    "N = len(images)\n",
    "K = len(means)\n",
    "\n",
    "assignments = [0]*N\n",
    "probs = [0]*N\n",
    "\n",
    "for i in range(N):\n",
    "    # Compute the score of data point i under each Gaussian component:\n",
    "    p = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        # YOUR CODE HERE (Hint: use multivariate_normal.pdf and rgb[i])\n",
    "        p[k] = ...\n",
    "        \n",
    "    # Compute assignments of each data point to a given cluster based on the above scores:\n",
    "    # YOUR CODE HERE\n",
    "    assignments[i] = ...\n",
    "    \n",
    "    # For data point i, store the corresponding score under this cluster assignment:\n",
    "    # YOUR CODE HERE\n",
    "    probs[i] = ...\n",
    "\n",
    "assignments = gl.SFrame({'assignments':assignments, 'probs':probs, 'image': images['image']})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the 'assignments' SFrame to find the top images from each cluster by sorting the datapoints within each cluster by their score under that cluster (stored in `probs`). We can plot the corresponding images in the original data using show().\n",
    "\n",
    "Create a function that returns the top 5 images assigned to a given category in our data (HINT: use the GraphLab Create function `topk(column, k)` to find the k top values according to specified column in an SFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_top_images(assignments, cluster, k=5):\n",
    "    # YOUR CODE HERE\n",
    "    images_in_cluster = ...\n",
    "    top_images = images_in_cluster.topk('probs', k)\n",
    "    return top_images['image']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use this function to show the top 5 images in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gl.canvas.set_target('ipynb')\n",
    "for component_id in range(4):\n",
    "    get_top_images(assignments, component_id).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These look pretty good! Our algorithm seems to have done a good job overall at 'discovering' the four categories that from which our image data was drawn. It seems to have had the most difficulty in distinguishing between rivers and cloudy skies, probably due to the similar color profiles of images in these categories; if we wanted to achieve better performance on distinguishing between these categories, we might need a richer representation of our data than simply the average [R G B] values for each image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Quiz Question:__ Which of the following images are *not* in the list of top 5 images in the first cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "![Images](chosen_images.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [dato]",
   "language": "python",
   "name": "Python [dato]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
